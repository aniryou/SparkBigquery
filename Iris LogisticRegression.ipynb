{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\"\"\"BigQuery I/O PySpark example.\"\"\"\n",
    "import json\n",
    "import pprint\n",
    "import subprocess\n",
    "import pyspark\n",
    "\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "MAP_FLOWER_NAME_TO_CODE = {\n",
    "    'Iris-setosa': 0,\n",
    "    'Iris-versicolor': 1,\n",
    "    'Iris-virginica': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the Google Cloud Storage bucket for temporary BigQuery export data used\n",
    "# by the InputFormat. This assumes the Google Cloud Storage connector for\n",
    "# Hadoop is configured.\n",
    "bq_project = \"savvy-aileron-127413\"\n",
    "fs_project = \"owners-681171445480\"\n",
    "bucket = \"spark_tmp\"\n",
    "account_email = \"savvy-aileron-127413@appspot.gserviceaccount.com\"\n",
    "train_input_directory = 'gs://{}/hadoop/tmp/bigquery/pyspark_input_train'.format(bucket)\n",
    "test_input_directory = 'gs://{}/hadoop/tmp/bigquery/pyspark_input_test'.format(bucket)\n",
    "localKeyfile = '/mnt/key.p12'\n",
    "\n",
    "# Output Parameters\n",
    "output_dataset = 'tutorial'\n",
    "output_table = 'wordcount_table'\n",
    "\n",
    "conf = {\n",
    "    # Input Parameters\n",
    "    'fs.gs.impl': 'com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem',\n",
    "    'fs.AbstractFileSystem.gs.impl': 'com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS',\n",
    "    'fs.gs.project.id': fs_project,\n",
    "    'mapred.bq.project.id': bq_project,\n",
    "    'mapred.bq.gcs.bucket': bucket,\n",
    "    'mapred.bq.temp.gcs.path': train_input_directory,\n",
    "    'mapred.bq.input.project.id': bq_project,\n",
    "    'mapred.bq.input.dataset.id': 'iris',\n",
    "    'mapred.bq.input.table.id': 'train',\n",
    "    'google.cloud.auth.service.account.enable': 'true',\n",
    "    'google.cloud.auth.service.account.email': account_email,\n",
    "    'google.cloud.auth.service.account.keyfile': localKeyfile,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_conf = conf\n",
    "\n",
    "# Load train-data in from BigQuery.\n",
    "train_table_data = sc.newAPIHadoopRDD(\n",
    "    'com.google.cloud.hadoop.io.bigquery.JsonTextBigQueryInputFormat',\n",
    "    'org.apache.hadoop.io.LongWritable',\n",
    "    'com.google.gson.JsonObject',\n",
    "    conf=train_conf)\n",
    "\n",
    "def featurize(x):\n",
    "    return LabeledPoint(MAP_FLOWER_NAME_TO_CODE[x['class']],[x['sl'],x['sw'],x['pl'],x['pw']])\n",
    "\n",
    "# extract LabeledPoint\n",
    "train_data = (\n",
    "    train_table_data\n",
    "    .map(lambda (_, record): json.loads(record))\n",
    "    .map(featurize)).collect()\n",
    "\n",
    "model = LogisticRegressionWithLBFGS.train(sc.parallelize(train_data), iterations=1e2, numClasses=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_conf = conf\n",
    "test_conf['mapred.bq.input.table.id'] = 'test'\n",
    "test_conf['mapred.bq.temp.gcs.path']  = test_input_directory\n",
    "\n",
    "# Load test data in from BigQuery.\n",
    "test_table_data = sc.newAPIHadoopRDD(\n",
    "    'com.google.cloud.hadoop.io.bigquery.JsonTextBigQueryInputFormat',\n",
    "    'org.apache.hadoop.io.LongWritable',\n",
    "    'com.google.gson.JsonObject',\n",
    "    conf=test_conf)\n",
    "\n",
    "# extract LabeledPoint\n",
    "test_data = (\n",
    "    test_table_data\n",
    "    .map(lambda (_, record): json.loads(record))\n",
    "    .map(featurize))\n",
    "\n",
    "\n",
    "def predictionError(model, data):\n",
    "    actualsAndPredictions = data.map(lambda p: (p.label, model.predict(p.features)))\n",
    "    error = actualsAndPredictions.filter(lambda (actual, prediction): actual != prediction).count() / float(data.count())\n",
    "    return error\n",
    "\n",
    "test_error = predictionError(model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n"
     ]
    }
   ],
   "source": [
    "print(test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually clean up the staging_directories, otherwise BigQuery\n",
    "# files will remain indefinitely.\n",
    "train_input_path = sc._jvm.org.apache.hadoop.fs.Path(train_input_directory)\n",
    "train_input_path.getFileSystem(sc._jsc.hadoopConfiguration()).delete(train_input_path, True)\n",
    "test_input_path = sc._jvm.org.apache.hadoop.fs.Path(test_input_directory)\n",
    "test_input_path.getFileSystem(sc._jsc.hadoopConfiguration()).delete(test_input_path, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
